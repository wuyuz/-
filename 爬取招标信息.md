## 爬取招标信息总结

#### 需求

- 公司大致要求如下：这是之前做的项目并未总结，以下是整个项目实现的过程

  ```
  http://ggzy.hebei.gov.cn/index.html 河北公共资源交易中心
  提取内容:成交公示中业务类型为工程建设中标信息
  
  提取要求:
      1.完整的html中标信息
      2.第一中标候选人
      3.中标金额
      4.中标时间
      5.其它参与投标的公司
  
  https://www.fjggfw.gov.cn/Website/JYXXNew.aspx 福建省公共资源交易中心
  提取内容:工程建设中的中标结果信息/中标候选人信息
      1. 完整的html中标信息
      2. 第一中标候选人
      3. 中标金额
      4. 中标时间
      5. 其它参与投标的公司
  ```

  

#### 要点总结

- 第一个项目：通过设置一个统一模板url，拼接url后深度爬取，触发新的解析函数，采用scrapy框架进行数据爬取

  ```python
  爬虫文件：
  # -*- coding: utf-8 -*-
  import scrapy
  from HeBei.items import HebeiItem
  
  class SpiderSpider(scrapy.Spider):
      name = 'spider'
      # allowed_domains = ['www.xx.com']
      # 起始网址
      start_urls = ['http://www.hebpr.gov.cn/hbjyzx/jydt/001002/001002002/001002002003/jyxxList.html']
  
      #模板url
      url = 'http://www.hebpr.gov.cn/hbjyzx/jydt/001002/001002002/001002002003/%d.html'
      pageNum = 1
  
      # 解析函数，用于解析本网页li标签
      def parse(self, response):
          li_list = response.xpath('//*[@id="content_001002002003"]/li')
          for li in li_list:
              title = li.xpath('./div/a/text()').extract_first()
              # 解析出二级页面的url
              detail_url = 'http://www.hebpr.gov.cn/'+li.xpath('./div/a/@href').extract_first()
              item = HebeiItem()
              item['title'] = title
              yield scrapy.Request(detail_url,callback=self.parse_detail,meta={'item':item})
  
          if self.pageNum<20:
              self.pageNum += 1
              new_url = self.url%self.pageNum
  
              yield scrapy.Request(new_url,callback=self.parse)
  
      def parse_detail(self,response):
          item = response.meta['item']
          name = response.xpath('/html/body/div[3]/div[2]/div/div[2]/div/table//tr[3]/td[2]/p//text() | /html/body/div[3]/div[2]/div/div[2]/div/table/tbody/tr[6]/td[2]/p/span[1]//text() | /html/body/div[3]/div[2]/div/div[2]/div/table/tbody/tr[3]/td/table//tr[3]/td[3]/div//text() | /html/body/div[3]/div[2]/div/div[2]/div/table[1]//tr[3]/td[3]/text()').extract()
          time = response.xpath('/html/body/div[3]/div[2]/div/div[2]/div/table//tr[11]/td[2]/p//text() | /html/body/div[3]/div[2]/div/div[2]/div/table/tbody/tr[10]/td[2]/p/span[1]//text() | /html/body/div[3]/div[2]/div/div[2]/div/table/tbody/tr[2]/td/table//tr[5]/td[1]//text() | /html/body/div[3]/div[2]/div/div[1]/div[1]/text()').extract()
          name = ''.join(name)
          item['name'] = name
          time = ''.join(time)
          item['time'] = time
          yield item
  
  --------------------------------------------------------
   #item.py文件
  import scrapy
  class HebeiItem(scrapy.Item):
      title = scrapy.Field()
      name = scrapy.Field()
      time = scrapy.Field()
  
  ---------------------------------------------------------
   #pipelines.py文件
  import pymysql
  
  class HebeiPipeline(object):
      def open_spider(self,spider):
          self.client = pymysql.Connect(host='127.0.0.1', port=3306, user="root", password='123', db='news')
  
      def process_item(self, item, spider):
          title = item['title']
          name = item['name']
          time = item['time']
          # print(name,title,time)
          sql = 'insert into data values ("%s","%s","%s")'%(name,title,time)
          self.cusor = self.client.cursor()
          try:
              self.cusor.execute(sql)
              self.client.commit()
          except Exception as e:
              print(e)
              self.cusor.rollback()
  
      def close_spider(self, spider):
          self.cusor.close()
          self.client.close()
  ```

- 第二个项目：由于该项目是通过ajax进行动态加载数据，所以我们使用request进行发送消息，注意：我们使用request，通过指定请求头，请求体来获得数据，最后对数据进行解析

  ```python
  import requests
  import random,re
  
  #UA伪装
  from lxml import etree
  
  headers = {
      'User-Agent' : 'Mozilla/5.0 (Linux; Android 4.2.1; en-us; Nexus 4 Build/JOP40D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Mobile Safari/535.19',
      'Cookie':'_qddamta_2852155767=4-0; Hm_lpvt_94bfa5b89a33cebfead2f88d38657023=1567769302; Hm_lpvt_63d8823bd78e78665043c516ae5b1514=1567769302; _qddagsx_02095bad0b=5b10f670f189576be11c7a486709ffd468e397833d0587e5c8f4b669a71310df10c8ce832c7d552547e4e5515f88cd44a5791cf82fb5c2da11331b365c1879e5e4c34fa796bdcee6dfe323f46a5b5214688dd78498a8f89a7db8eeeffa7353844e6ce8018e3b8be6363d29621f28273af76d6baaf1f39a8b16c033a82e9c4a5d'
      }
  
  #使用session来保存cooike，注意向同一个网址发送请求时使用session会自动保存cookie，之后发送请求就用session就可以了
  # session = requests.Session()
  
  url = 'https://www.fjggfw.gov.cn/Website/AjaxHandler/BuilderHandler.ashx'
  
  def page(num):
      url_data = {
          'OPtype': 'GetListNew',
          'pageNo': num,
          'pageSize': '10',
          'proArea': '-1',
          'category': 'GCJS',
          'announcementType': '5',
          'ProType': '-1',
          'xmlx': '-1',
          'projectName': '',
          'TopTime': '2019-06-07 00:00:00',
          'EndTime': '2019-09-05 23:59:59',
          'rrr': random.random(), #此为随机数，可有可无
      }
  
  
      #必须携带cookie，且注意在headers中必须大写，且必须提交数据，否则无法获得数据,获得后反序列化
      page_response = requests.post(url=url,headers=headers,data=url_data).json()
      for item in page_response.get('data'):
          print(item)
          #公告类型
          title = item.get('TITLE')
          if title == '中标结果公告':
              ID = item.get('M_ID')
              GGTYPE = item.get('GGTYPE')
  
              #调用二级网页
              detial_page(ID,GGTYPE)
  
  def detial_page(ID,GGTYPE):
  
      params = {
          'OPtype': 'GetGGInfoPC',
          # 传入的还是int类型
          'ID': int(ID),
          'GGTYPE': GGTYPE,
          'url': 'AjaxHandler / BuilderHandler.ashx',
      }
  
      response = requests.get(url,params=params,headers=headers).json()
  
      #各种类型
      tender_status = response.get('node')
      for temp in tender_status:
          if temp.get('TITLE') == '中标结果公告':
              time = temp.get('TM')
              num = temp.get('NUM')
              all_html = response.get('data')[int(num)]
              # tree = etree.HTML(all_html)
              # 有问题？？？？？？？？？？？
              # zhongbiao = tree.xpath('.//text()')
              print(all_html)
              ret = re.findall(r'中标供应商为：</span></span><span style=";font-family:宋体;font-size:19px"><span style="font-family:宋体">(?P<company>\w+)</span',all_html,re.S)
              print(ret)
  
  page(2)
  ```

  