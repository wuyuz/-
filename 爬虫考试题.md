## 爬虫考试题

1.简述cookie的概念和作用 

```
保存在客户端的键值对
保存客户端相关状态，提供个性化服务。也是用户行为的工具。
```



2.简述scrapy各个核心组件之间的工作流程

```
下载器：用于下载网路上的资源
管道：持久化存储，验证实体有效性，请求不需要数据
调度器：筛选重复网页请求，将筛选过的网页请求放入队列中
爬虫文件：用于提取网页中自己需要信息，发起请求，解析响应
引擎：框架的核心。控制调度器，下载器，爬虫
```



3.基于crwalSpider实现数据爬取的流程

```
创建工程：scrapy startproject xxxx
创建一个基于CrawlSpider爬虫文件：
scrapy genspider -t crawl spiderName www.xxx.com
构造链接提取器和规则解析器。
xpath解析每页网页，如果解析详情页定义方法手动发送请求，并进行请求传参
setting配置属性：
items.py创建表单。
管道实现数据持久化存储
```



4.在scrapy中如何实现将同一份数据值存储到不同的数据库中

```
settings配置ITEM_PIPELINES（值越小，优先级越大），
管道文件定义类定义方法open_spider(用于打开文件/连接数据库),close_spider(用于关闭文件/关闭数据库),当优先
级大的执行完，应在process_item 返回item,此时会将数据传到下一个类进行持久化存储
```



5.scrapy的下载中间件的作用以及类中重点方法的使用介绍

```
process_request拦截请求
User-Agent伪装。更改请求头
更换代理IP
process_exception拦截异常
发生异常请求，如IP被封，更换代理IP，将修正后request请求重新发送
process_response拦截响应
动态加载的新闻，通过结合selenium，拿取网页信息
```



6.scrapy的pipeline的作用及其工作原理

```
用来接收爬虫文件提交item对象
process_item方法书写存储数据代码，每接收一次item就会调用一次
open_spider用于连接数据库/打开文件，爬虫开始时候才会执行一次
close_spider关闭数据库/文件，爬虫结束执行
```



7.有关scrapy的pipeline中的process_item方法的返回值有什么注意事项

```
item是返回给了下一个即将被执行的管道类，如果有下一个管道类，必须书写returin item否则下一个管道类接收不到信
息
```



8.scrapy实现持久化存储有几种方式，如何实现  

```
基于文件存储 f=open()
基于关系型数据库存储mysql
基于非关系型数据库存储MongoDB,redis
```



9.描述使用xpath实现数据解析的流程

```
实例化etree类，将页面源码加载对象中
结合XPATH表达式对标签进行定位和数据提取
定位：标签定位，属性定位，索引定位
数据提取：/text()
//text()
/@href
```



10.你如何处理相关动态加载的页面数据

```
打开网页F12打开抓包工具，重载页面，ctrl+F全局搜索页面字样。查询到的文件点击查看response，再ctrl+F，在
response对象搜索有的话，访问当前这个URL
```



11.如何实现分布式？简述其实现和部署流程

```
1.创建一个工程：scrapy startproject fbsPro
2.创建spider:scrapy genspider -t crawl fbs www.xxx.com
3.在spider导入包：from scrapy_redis.spiders import RedisCrawlSpider
4.将当前爬虫类父类修改成RedisCrawlSpider
5.将start_urls替换成redis_key = 'xxx'表示可被共享调度器中队列的名称
6.编写爬虫类爬取数据操作 区分RedisSpider/RedisCrawlSpider
7.settings配置（redis的IP,PORT，配置调度器，增加去重容器，指定管道）
ITEM_PIPELINES = {
 'scrapy_redis.pipelines.RedisPipeline': 400
}
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True
REDIS_HOST = ' 192.168.11.194'  #本机IP
REDIS_PORT = 6379
8.redis配置文件更改
9.启动redis客户端，服务端
10.cd爬虫文件执行程序，scrapy runspider xxx.py
11.向调度器队列扔入其实URL
```



12.谈谈你对https数据加密方式的理解

```
对称密钥加密：客户端加密，将加密后的信息和密钥一起发送给服务器端，服务器端拿到加密后的信息和密钥，进行解密。
非对称密钥加密：服务器创建密钥，发送给客户端公钥，客户端加密，将密文发送给服务器端
证书密钥加密方式：第三方机构 ，经过防伪的公钥
```



13.原生的scrapy框架为什么不可以实现分布式？

```
调度器无法被分布式机群共享
管道无法被共享
```



14.常见的反爬机制有哪些？如何进行处理？  

```
1.通过User-Agent反爬
2.通过Referer来反爬
3.通过Cookie来反爬
4.通过js加密请求参数、解密加密的数据来反爬
5.通过行为验证来反爬
6.通过ip地址来反爬
7.通过自定义字体来反爬
8.ip访问频次。
9.css反爬
```



1. 在爬虫中如何实现数据清洗（三种清洗方法）

```
isnull()和notnull()删选出空值/非空值
dropna() 过滤掉有NaN的行
fillna()  可以填充数值，一般填充附近列或行的值
```

